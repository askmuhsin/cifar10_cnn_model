{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "convenient-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "senior-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds_toolkit.general_utils.gpu_utils import addGPU\n",
    "\n",
    "addGPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "delayed-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outside-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import CIFAR10_dataset\n",
    "from utils.misc import (\n",
    "    show_random_images,\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    is_cuda,\n",
    "    show_model_summary,\n",
    "    show_img_grid,\n",
    "    show_random_images_for_each_class,\n",
    "    get_wrong_predictions\n",
    ")\n",
    "from utils.model import Net\n",
    "from utils.training import train\n",
    "from utils.testing import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "billion-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "latin-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "cuda = is_cuda()\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "indoor-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CIFAR10_dataset(\n",
    "    train=True, cuda=cuda\n",
    ").get_loader()\n",
    "test_loader = CIFAR10_dataset(\n",
    "    train=False, cuda=cuda\n",
    ").get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "republican-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "possible-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "visible-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "show_model_summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "multiple-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 120\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "imposed-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    notes=\"\"\"In this run dilated conv is fixed.\"\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.2, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "constitutional-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "absolute-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "show_model_summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "monthly-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 120\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "foster-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    name=\"b1_b2_more_kernels\",\n",
    "    notes=\"\"\"More kernels on Block 1 and Block 2. Brining total params to 179k+\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.2, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "jewish-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    name=\"b1_b2_more_kernels\",\n",
    "    notes=\"\"\"More kernels on Block 1 and Block 2. Brining total params to 179k+\"\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.2, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "amateur-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "regular-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "show_model_summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "divine-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 120\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "infinite-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    name=\"lr_plateau_factor_0p5\",\n",
    "    notes=\"\"\"LR Plateau factor to 0.5 No change in model.\"\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "plain-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "successful-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "show_model_summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "handmade-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 120\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "attempted-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    name=\"dropout_to_0p1\",\n",
    "    notes=\"\"\"Reduce dropout value to 0.1\"\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "actual-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "material-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "show_model_summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "freelance-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 120\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "illegal-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    name=\"inc_b3_ksize_dec_b1_ksize\",\n",
    "    notes=\"\"\"Increase Block 3 kernel size, and reduce Block kernel size. Total params to 195280\"\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "million-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "show_model_summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "powerful-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "show_model_summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "tender-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 120\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "young-hampton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">progressive_kernel_size_blks</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/askmuhsin/cifar_10_lte200k_npool\" target=\"_blank\">https://wandb.ai/askmuhsin/cifar_10_lte200k_npool</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/askmuhsin/cifar_10_lte200k_npool/runs/1pfizgeb\" target=\"_blank\">https://wandb.ai/askmuhsin/cifar_10_lte200k_npool/runs/1pfizgeb</a><br/>\n",
       "                Run data is saved locally in <code>/data/omnibox1/muhsin/OOB_experiments/eva/cifar10_cnn_model/training_nbs/wandb/run-20210621_003735-1pfizgeb</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    name=\"progressive_kernel_size_blks\",\n",
    "    notes=\"\"\"Progressive kernel size on all blocks. Total params to 194,104\"\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
