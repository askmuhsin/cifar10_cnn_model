{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assigned-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broke-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds_toolkit.general_utils.gpu_utils import addGPU\n",
    "\n",
    "addGPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "heavy-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "running-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import CIFAR10_dataset\n",
    "from utils.misc import (\n",
    "    show_random_images,\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    is_cuda,\n",
    "    show_model_summary,\n",
    "    show_img_grid,\n",
    "    show_random_images_for_each_class,\n",
    "    get_wrong_predictions\n",
    ")\n",
    "from utils.model import Net\n",
    "from utils.training import train\n",
    "from utils.testing import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "agreed-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "challenging-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "cuda = is_cuda()\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "embedded-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CIFAR10_dataset(\n",
    "    train=True, cuda=cuda\n",
    ").get_loader()\n",
    "test_loader = CIFAR10_dataset(\n",
    "    train=False, cuda=cuda\n",
    ").get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cheap-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "removed-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adaptive-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "show_model_summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "proud-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 120\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "based-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    notes=\"\"\"In this run dilated conv is fixed.\"\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.2, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aquatic-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "handmade-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "show_model_summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "double-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 120\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fifth-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    name=\"b1_b2_more_kernels\",\n",
    "    notes=\"\"\"More kernels on Block 1 and Block 2. Brining total params to 179k+\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.2, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "athletic-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    name=\"b1_b2_more_kernels\",\n",
    "    notes=\"\"\"More kernels on Block 1 and Block 2. Brining total params to 179k+\"\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.2, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fewer-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "southern-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "show_model_summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "accurate-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 120\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "surrounded-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    name=\"lr_plateau_factor_0p5\",\n",
    "    notes=\"\"\"LR Plateau factor to 0.5 No change in model.\"\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vocal-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "norman-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "show_model_summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fiscal-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 120\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "chubby-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    name=\"dropout_to_0p1\",\n",
    "    notes=\"\"\"Reduce dropout value to 0.1\"\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "simplified-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "democratic-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "show_model_summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "crucial-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 120\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "checked-array",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">inc_b3_ksize_dec_b1_ksize</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/askmuhsin/cifar_10_lte200k_npool\" target=\"_blank\">https://wandb.ai/askmuhsin/cifar_10_lte200k_npool</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/askmuhsin/cifar_10_lte200k_npool/runs/ltcrsoyx\" target=\"_blank\">https://wandb.ai/askmuhsin/cifar_10_lte200k_npool/runs/ltcrsoyx</a><br/>\n",
       "                Run data is saved locally in <code>/data/omnibox1/muhsin/OOB_experiments/eva/cifar10_cnn_model/training_nbs/wandb/run-20210620_233958-ltcrsoyx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='cifar_10_lte200k_npool', \n",
    "    entity='askmuhsin', \n",
    "    reinit=True,\n",
    "    name=\"inc_b3_ksize_dec_b1_ksize\",\n",
    "    notes=\"\"\"Increase Block 3 kernel size, and reduce Block kernel size. Total params to 195280\"\"\"\n",
    ")\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=0.015, momentum=0.9\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, \n",
    "    patience=3, min_lr=0.0001, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    print(f\"[EPOCH {epoch} / {EPOCHS}] -- \")\n",
    "    train_loss_batch, train_acc_batch = train(\n",
    "        net, device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        lr_scheduler\n",
    "    )\n",
    "    train_acc = np.mean(train_acc_batch)\n",
    "    train_loss = np.mean(train_loss_batch)\n",
    "    test_loss, test_acc = test(\n",
    "        net, device, \n",
    "        test_loader,\n",
    "        lr_scheduler\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for metric in ['train_loss', 'train_acc', 'test_loss', 'test_acc', 'epoch']:\n",
    "        results[metric].append(eval(metric))\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
